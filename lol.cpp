
//small neural network and testing.
#include <bits/stdc++.h>
#define fr freopen("input.in","r",stdin)
#define fw freopen("output.out","w",stdout)
#define iOs ios_base::sync_with_stdio(false);
#define INF 111313131
#define all(x) (x).begin(), (x).end()
#define debug cout<<"here\n"
#define debugin cout<<"inhere\n"
#define debugname cout<<"dharmang\n";
using namespace std;
#define pb push_back
#define mp make_pair
#define sc second
#define fir first
#define No_Of_Inputs 2
#define No_Of_Layer 3
#define HI 0.5
#define LO -0.5
typedef long long LL;
typedef pair<int, int> pii;
typedef vector<int> vi;
typedef vector<vi> vvi;
vector< vector<double> > inpweight,hidweight,layer; 
vector<double> lol,col,pol;
//sigmoid function used is pow((1+pow(e,-x)),-1);
bool flag1=false,flag2=false; int counter=0;
int main()
{
    fw;fr;
    iOs;
    cout<<"The Network has 3 layer\n2 computing units in first layer\n3 computing units in 2nd later \n and 2 computing units in third layer\n";
    //initializing layer inputs. Setting the computing units at 0 
    lol.pb(0); lol.pb(0); 
    layer.pb(lol); lol.pb(0); layer.pb(lol); lol.clear(); 
    lol.pb(0); lol.pb(0); layer.pb(lol); 
    cout<<layer[0].size()<<" "<<layer[1].size()<<" "<<layer[2].size()<<endl<<endl<<endl<<endl;
    //getting the input fot the neural network as a binary only 
    //**************************************************************************************
    //The main purpose of this neural net is to make input equals output..
    //**************************************************************************************
    for(int i=0;i<layer[0].size();i++)
    {
        layer[0][i]=(double
        )(rand()%2); cout<<layer[0][i]<<" "<<i<<"th input\n";
    }
    //putting numbe rif weights from input layer of the neural net
    vector<int> just; just.pb(3); just.pb(3);
    //putting number of weights from the hidden layer of the neural net
    //Initializing all the weights present in the neural net
    //Initialising weights generated by the input layer
    for(int i=0;i<just.size();i++)
    {
        for(int j=0;j<just[i];j++)
        {
            pol.pb( LO + static_cast <double
            > (rand()) /( static_cast <double
            > (RAND_MAX/(HI-LO))));
        }
        inpweight.pb(pol);
        pol.clear();
    }
    //Initialising weights generated by the hidden layer
    just.clear(); just.pb(2); just.pb(2); just.pb(2);
    for(int i=0;i<just.size();i++)
    {
        for(int j=0;j<just[i];j++)
        {
            pol.pb( LO + static_cast <double
            > (rand()) /( static_cast <double
            > (RAND_MAX/(HI-LO))));
        }
        hidweight.pb(pol);
        pol.clear();
    }  
    //Propagating through the neural net and putting all the computing units values obtained during this 
    // propagation
    wherelifetakesme:
    long double summer=0.0;
    //getting values of hidden layers
    for(int i=0;i<3;i++)
    {
        summer=0.0;
        for(int j=0;j<2;j++)
        {
            summer+=((double)layer[0][j]*(double)inpweight[j][i]);
        }
        layer[1][i]=pow(1+exp(-(summer)),-1);
    }
    //getting values of output layers
    for(int i=0;i<2;i++)
    {
        summer=0.0;
        for(int j=0;j<3;j++)
        {
            summer+=((double)layer[1][j]*(double)hidweight[j][i]);  
        }
        layer[2][i]=pow(1+exp(-(summer)),-1);   
    }
    cout<<"Output Of Layers:\n";
    for(int i=0;i<layer.size();i++)
    {
        cout<<i+1<<"th layer: ";
        for(int j=0;j<layer[i].size();j++)
        {
            cout<<layer[i][j]<<" ";
        }
        cout<<endl;
    }
    //Checking if the outputs are desired values or not
    //If desired, we make the flag as true
    int tmp1 = layer[0][0];
    int tmp2 = layer[2][0];
    //if((int)layer[0][0]==(int)layer[2][0])
    if(tmp1 == tmp2)
        { 
        debug; debugname;
        cout<<"CHecking i f both are equal\n";
        cout<<tmp1<<" "<<tmp2<<endl;
            flag1=true;
        }
    if((int)layer[0][1]==(int)layer[2][1])
        {
            flag2=true;
        }

    if(flag1==false or flag2==false)
    {
        counter++; cout<<"Training Number: "<<counter<<endl;
        //training the net until we get the desired output
        //Applying Delta Rule for removing the inconsistencies in weights of the network
        double opgrad[2];
         memset(opgrad,0.0,sizeof(opgrad));
        //error gradient for the output layer
        for(int i=0;i<2;i++)
        {
            opgrad[i]=(double)((double)(layer[2][i]-layer[0][i])*(double)((double)1-layer[2][i])*(double)layer[2][i]);
            //cout<<opgrad[i]<<" output gradient ";
        }
        cout<<endl;
        cout<<"Error Gradient for output layer:\n";
        for(int i=0;i<2;i++) cout<<opgrad[i]<<" ";
        cout<<endl;
        //error gradient for the middle hidden layer
        cout<<" hidden layer gradient \n";
        long double tot=0,hidgrad[3]; 
        memset(hidgrad,(float)0.0,sizeof(hidgrad));
        int l=0;
        for(int i=0;i<layer[1].size();i++)
        {
            double tot=0.0;
            for(int j=0;j<hidweight[i].size();j++)
            {
                tot+=(double)hidweight[i][j]*(double)opgrad[j];
            }
            hidgrad[i]=(double)tot*(double)layer[1][i]*(double)((double)1-layer[1][i]);
        }
        for(int i=0;i<3;i++)
         cout<<hidgrad[i]<<" ";
         cout<<endl;
        //all error gradients found
        //Now going towards finding the change in weights of all the connections of computing units
        //learning rate is the size of jumps that we will have on the graph of our network so that we 
        //achieve out desired output
        double learningrate=0.5;
        for(int i=0;i<inpweight.size();i++)
        {
            for(int j=0;j<inpweight[i].size();j++)
            {
                inpweight[i][j]-=double(learningrate)*(double)layer[0][i]*(double)hidgrad[j];
            }
        }
        for(int i=0;i<hidweight.size();i++)
        {
            for(int j=0;j<hidweight[i].size();j++)
            {
                hidweight[i][j]-=double(learningrate)*(double)layer[1][i]*(double)opgrad[j];
            }
        }
        if(counter<10000)
        goto wherelifetakesme;
        //goto is used to apply these weight changes and to check whether the network npw needs training
        //or not    
    }
    cout<<"Weights in between input and hidden layer:\n";
    for(int i=0;i<inpweight.size();i++)
    {
        for(int j=0;j<inpweight[i].size();j++)
        {
            cout<<inpweight[i][j]<<" ";
        }
        cout<<endl;
    }
    cout<<"Weights in between hidden and output layers: \n";
    for(int i=0;i<hidweight.size();i++)
    {
        for(int j=0;j<hidweight[i].size();j++)
        {
            cout<<hidweight[i][j]<<" ";
        }
        cout<<endl;
    }
    return 0;
}